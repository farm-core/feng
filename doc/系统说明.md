# 系统说明
### 一、注册中心
+ #### 概述
    服务注册发现通过spring cloud eureka ，eureka注册中心通过集群部署保证服务发现高可用。
+ #### 服务端配置：
    + 服务端默认端口:8761，默认开启自身注册
    + 开启自我保护模式，其中RenewalPercentThreshold使用默认设置0.85。
    + 注册服务地址使用服务器实际IP，在使用docker容器情形下，配置docker容器网络模式为host,使容器IP与宿主机IP一致
      或者单独配置docker网卡设置指定的IP地址。
    + 通用eureka服务端配置方式：
    ```
    server:
      port: ${SERVER_PORT:8761}
    eureka:
      instance:
        prefer-ip-address: true
        instance-id: ${spring.cloud.client.ipAddress}:${spring.application.name}:${server.port}:@project.version@
      client:
        register-with-eureka: false
        fetch-registry: false
        service-url:
          defaultZone: http://${EUREKA_NODE_1_HOST:localhost}:${EUREKA_NODE_1_PORT:8761}/eureka/,http://${EUREKA_NODE_2_HOST:localhost}:${EUREKA_NODE_2_PORT:8761}/eureka/
    spring:
      application:
        name: eureka
    ```
+ ####配置说明：
  + instance-id:自定义生成，规则为服务运行宿主机或容器IP+服务名称+服务运行端口+服务版本号 \
    服务版本号读取服务POM中version信息，读取pom内信息前提，需在POM中配置项目resource路径 \
    配置方式在maven配置中详细说明；
  + defaultZone: {EUREKA_NODE_1_HOST:localhost}:${EUREKA_NODE_1_PORT:8761} \
    通过读取环境变量配置地址
  + 自我保护模式说明: 生产环境建议开启，防止出现注册服务列表失效；测试环境可以关闭，便于服务注册频繁注册注销；\
    生产环境中触发自我保护后，可使用eureka提供的rest接口发送delete删除无效的服务。服务列表建议定期巡检；
    下线服务请求示例：
    ```
    DELETE /eureka/apps/EUREKA/10.121.18.181:eureka:8761:0.1-alpha HTTP/1.1
    Host: 10.150.18.18:8761
    
    DELETE /eureka/apps/{appName}/{instanceId} HTTP/1.1
    Host: 注册中心地址
    ```
     
#### 客户端配置
+ 服务ID规则：
`${spring.cloud.client.ipAddress}:${spring.application.name}:${server.port}:@project.version@`\
 即：服务ip:服务名称:服务端口:服务版本号
    
+ 服务名称规范： 
  + {顶级项目名称}-{子模块名称}-{服务类型} 举例：cmacast-gps-service
  + 服务名称强制小写字母，包含具体含义，使用单数名词形式。
  + 服务类型：
     + Service：数据服务，该服务与数据库直接交互
     + Server：提供特性服务，服务不参与数据库读写。
     + Center：代表统一服务，提供全局类服务。
     + Config: 配置文件服务
#### 其他
+ 服务离线：服务离线强制使用shutdown接口停止服务，非正常停止服务使用eureka提供的rest接口注销相关服务。
+ 服务注册：心跳检测时长，服务获取注册表时间，服务代理等均使用默认配置。
---------------------------------------------------
### 二、服务熔断与负载均衡（服务间）
+ #### 负载均衡熔断配置
```
feign:
  httpclient:
    enabled: false
  okhttp:
    enabled: true
  hystrix:
    enabled: true
ribbon:
#  ribbon请求连接的超时时间
  ConnectTimeout: 300
#  请求处理的超时时间
  ReadTimeout: 35000
#  配置所有请求都进行重试 如果为false则只对get请求重试
  OkToRetryOnAllOperations: true
# 当前实例全部失败后可更换实例次数
  MaxAutoRetriesNextServer: 2
# 当前实例只重试次数
  MaxAutoRetries: 0
# 使用OKhttpFeignClient
  okhttp:
    enabled: true
# 饥饿加载 避免第一次请求创建负载均衡Clients时间超时
  eager-load:
    enabled: true
    clients: cmacast-hydr-service,cmacast-ship-service,..
  http:
    client:
      enabled: false
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 60000
```
+ 此处使用okhttp客户端代替默认httpclient，以上为配置文件方式配置.
+ 也可以自定义feignclientConfig配置，在启动类注解中引入配置`@EnableFeignClients(defaultConfiguration = FeignClientConfigCustom.class)` \
  示例：
    ```
    @Configuration
    public class FeignClientConfigCustom {
    
        /**
         * 配置 Feign 日志级别
         * <p>
         * NONE：没有日志
         * BASIC：基本日志
         * HEADERS：header
         * FULL：全部
         * <p>
         * 配置为打印全部日志，可以更方便的查看 Feign 的调用信息
         *
         * @return Feign 日志级别
         */
        @Bean
        public Logger.Level feignLoggerLevel() {
            return Logger.Level.BASIC;
        }
    
        /** 自定义重试Retryer
         * maxAttempts feignClient重试次数
         * @author zhanggk
         * 2019/8/5 16:03
         */
        @Bean
        public Retryer retryer() {
            return new Retryer.Default(100,1000,1);
        }
    
        /**
         * 请求建立链接超时设置 响应超时设置
         * @author zhanggk
         */
        @Bean
        Request.Options feignOptions() {
            return new Request.Options(200, 30000);
        }
    }
    ```

  Openfeign向下兼容feign，支持原有注解与配置方式，方法单独配置熔断：hystrix.command.methodName.execution
  
+ #### 重试次数配置：
  + 通过MaxAutoRetriesNextServer配置重试实例数量
  + 通过MaxAutoRetries配置当前实例重试次数
  + 以及最外层的feign MaxAttempts 设置feign重试次数，
  
+ #### 重试超时机制  
  + ribbon与feign都有重试超时机制，在Greenwich版本中不建议配置feign客户端重试，\
    统一由ribbon来管理请求重试与请求超时，避免出现重试配置混乱，导致请求异常中断。
  + 重试次数计算公式： `(MaxAutoRetries+1)*(MaxAutoRetriesNextServer+1) * (feign[maxAttempts])= 最终重试次数`
  + OkToRetryOnAllOperations 对于POST请求没有做幂等的接口谨慎开启，避免表单提交造成重复数据。
  + 超时机制，hystrix超时作用于一次调用最外层，ribbon超时为调用内层超时，所以hystrix的超时时间要大于 (1 + MaxAutoRetries + MaxAutoRetriesNextServer) * ReadTimeout(ribbon) \
    ribbon的超时配置最后会应用到实际调用的http请求，参考源码：
    
    ```
    public class OkHttpLoadBalancingClient extendsAbstractLoadBalancingClient<OkHttpRibbonRequest, OkHttpRibbonResponse, OkHttpClient> {
            ...
            ...
    	OkHttpClient getOkHttpClient(IClientConfig configOverride, boolean secure) {
    		IClientConfig config = configOverride != null ? configOverride : this.config;
    		RibbonProperties ribbon = RibbonProperties.from(config);
    		OkHttpClient.Builder builder = this.delegate.newBuilder()
    				.connectTimeout(ribbon.connectTimeout(this.connectTimeout),
    						TimeUnit.MILLISECONDS)
    				.readTimeout(ribbon.readTimeout(this.readTimeout), TimeUnit.MILLISECONDS)
    				.followRedirects(ribbon.isFollowRedirects(this.followRedirects));
    		if (secure) {
    			builder.followSslRedirects(ribbon.isFollowRedirects(this.followRedirects));
    		}
    
    		return builder.build();
    	}
    ```
  
+ #### 负载均衡：
  + Openfeign本身集成了Eureka、Ribbon 默认实现了负载均衡。
  + 使用spring.cloud.loadbalancer.retry.enabled配置全局负载均衡重试是否启动。
  + 配置解读
    + 通过配置ribbon.okhttp.enabled=true 触发 org.springframework.cloud.openfeign.ribbon.OkHttpFeignLoadBalancedConfiguration \
      ribbon中使用okhttp执行类org.springframework.cloud.netflix.ribbon.okhttp.RetryableOkHttpLoadBalancingClient \
      使用httpclient同理可以在org.springframework.cloud.netflix.ribbon包下的apache内找到对应的实现逻辑。

+ #### 其他
  + 重试与熔断机制在此版本中配置较为混乱，官方文档中说明也过于简略，网上资料因为不同版本的作用机制不同难以作为参考。在开发配置中尽量做完整测试验证是否生效。
  + 后期如果单接口并发过高，可以考虑应用Hystrix请求合并，实现HystrixCollapser接口。
 
------------------------------------ 
### 三、服务调用链路跟踪

+ #### 概述：
链路跟踪信息收集使用spring-cloud-zipkin收集tracer信息，依赖

    <dependency>
       <groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-zipkin</artifactId>
    </dependency>
    
+ #### 配置示例：
```
spring:
  zipkin:
    rabbitmq:
      queue: zipkin
    locator:
      discovery:
        enabled: true
  sleuth:
    sampler:
      probability: 1.0F
    baggage-keys:
      - record_info
      - request_ip
```      
 1. Tracer信息发送使用zipkin2.reporter.amqp.RabbitMQSender或继承此类发送至rabbitmq，默认queue名zipkin。
 2. baggage-keys调用链路上下文额外信息，用于服务间调用信息透传。
 3. brave.propagation.ExtraFieldPropagation工具类对baggage内容进行更改，工具类维护brave.propagation.TraceContext
    本质为map，透传信息key必须在配置文件中声明。
+ #### tracer信息持久化
  + 持久化流程：sleuth->rabbitSender(zipkin client)->zipkin(server)->elasticsearch
  + 启动依赖服务：rabbitmq  elasticsearch  zipkin-server
  + zipkin-server相关配置:
    + 命令示例：
    ```
     nohup java -jar -Xms1024m -Xmx1024m -Xmn640m -XX:SurvivorRatio=3 zipkin.jar \
     --RABBIT_ADDRESSES=10.150.18.18:5672 --RABBIT_USER=admin  --RABBIT_PASSWORD=admin \
     --ES_PIPELINE=zipkin-record-convert \
     --STORAGE_TYPE=elasticsearch  --ES_HOSTS=http://10.110.18.173:9200
    ```
    其中配置了rabbit与elasticsearch服务地址，zipkin-record-convert 为zipkin索引处理pipeline用于解析原始span信息
    pipeline信息可以通过GET _ingest/pipeline 在ES集群中查看或者编辑，增加新字段处理；
    ```
      "zipkin-record-convert" : {
        "description" : "tags record pipeline convert",
        "processors" : [
          {
            "convert" : {
              "field" : "tags.record",
              "target_field" : "record",
              "type" : "long",
              "ignore_missing" : true
            }
          },
          {
            "set" : {
              "field" : "user",
              "value" : "{{tags.user}}"
            }
          }
        ]
      }
    ```
      zipkin-server在GitHub上有详细的说明文档，各种参数配置均有示例。
    + zipkin 索引设置，在ES中配置了zipkin索引模板用于覆盖zipkin-server默认索引mapping，可通过GET _cat/templates在ES\
      集群内查看全部索引模板设置，模板依据正则匹配索引名称生效。
    + zipkin 服务运行默认端口9411，如果需要查看调用链路，需要定时运行zipkin-dependency服务，一般是每天运行一次，在GitHub中有详细文档。
--------------------------------------

### 四、服务运行监控
 + ####概述 \
   服务监控使用spring boot admin作为基础，具体模块对应monitor服务,服务端大部分采用默认配置。\
   客户端加入actuator, admin-client依赖,提供actuator端点（Endpoints）
 + ####配置说明：
   + 客户端通用配置：
   ```
   management:
     endpoints:
       web:
         exposure:
           include: '*'
     endpoint:
       health:
         show-details: always
     health:
       rabbit:
         enabled: false
   ```
   1. management.endpoints.web.exposure.include 此处配置需要服务暴露的端点，* 为全部接口都可以被admin服务访问的端点，\
      actuator端点列表可以通过访问服务 GET http://10.110.18.171:12002/actuator 获取，从中选取需要的端点。
      其中shutdown等敏感端点可以选择不暴露。
   2. management.endpoints.web.exposure.exclude 此处为配置需要服务排除的端点，配置方式与上面类似。
   3. health下配置服务内健康检查项。
   + 服务端配置，下面是远程监控服务配置方式：
   ```
   server:
     port: ${SERVER_PORT:10099}
     servlet:
       context-path: /monitor
   spring:
     application:
       name: monitor
     main:
       allow-bean-definition-overriding: true
     boot:
       admin:
         ui:
           public-url: http://host:12100
           title: 数据中心服务管理
   ```
   1. context-path tomcat的上下文路径，此处配置用于通过网关请求路由跳转,网关根据context-path路径进行路由。
   2. boot.admin.ui.public-url为admin服务端的访问地址，配置为本机实际host地址即可。
   3. title 页面显示标题，不影响实际功能。
   4. 其余配置可在源码中配置参数类de.codecentric.boot.admin.server.config.AdminServerProperties中查看，或查看官方文档。
   
### 五、网关以及鉴权服务   
### 五、配置文件
+ #### config服务
  + 配置中心端口9999，正式环境读取中心配置，根据git分支选择执行环境。可以在运行时配置环境变量CONFIG_LABEL切换分支。
+ #### 本地配置
  + 本地配置须设置 `spring.cloud.config.enabled=false` 避免远程配置文件覆盖本地。
### 六、项目编译
+ ####maven设置
  + maven settings中配置远程镜像地址，server标签配置远程maven服务器账号，远程docker镜像账号; 配置本地repositories;
+ ####POM文件编写  
  + maven pom配置远程maven repository, maven plugin repository，分别用于下载第三方依赖以及maven插件;
  + 第三方依赖版本通过根POM中的dependencyManagement标签配置，保持全部子模块依赖保持一致。
  + 项目pom配置注意事项:
    + 标签 `<artifactId> <name>` 不能省略，标签内容保持一致，在使用脚本部署时，`<artifactId> <name> `与模块文件夹、`spring.application.name`四者保持\
      一致用于脚本构建项目，运行容器。
    + 正式环境`<version>`标签需按照规范制定版本号，版本号配置影响实例eureka instanceId生成。
    + `<build>`子标签 `<finalName>datacenter-${name}</finalName>` 按照此种方式配置，统一jar包前缀，在脚本中作为运行jar包的名称。  
    + `<build>`子标签配置：
       ```
          <resources>
              <resource>
                  <directory>src/main/resources</directory>
                  <filtering>true</filtering>
              </resource>
          </resources>
       ```
       此处配置resources路径，用于yml配置文件读取POM中标签
       
+ ####maven plugin    
  + 必备插件： **maven-compiler-plugin  spring-boot-maven-plugin maven-antrun-plugin**
  + 配置说明 
    ```
        编译插件
         <plugin>
             <groupId>org.apache.maven.plugins</groupId>
             <artifactId>maven-compiler-plugin</artifactId>
             <version>3.1</version>
             <configuration>
                 <source>${java.version}</source>
                 <target>${java.version}</target>
             </configuration>
         </plugin>
         springboot 生成可执行jar包插件，注意配置mainClass启动类
         <plugin>
             <groupId>org.springframework.boot</groupId>
             <artifactId>spring-boot-maven-plugin</artifactId>
             <configuration>
                 <mainClass>com.clear.admin.AdminApplication</mainClass>
                 <fork>true</fork>
             </configuration>
             执行repackage接口
             <executions>
                 <execution>
                     <goals>
                         <goal>repackage</goal>
                     </goals>
                 </execution>
             </executions>
         </plugin>
         antrun用于打包后文件移动
         <plugin>
             <artifactId>maven-antrun-plugin</artifactId>
             <executions>
                 <execution>
                     package命令执行此插件run方法
                     <phase>package</phase>
                     <goals>
                         <goal>run</goal>
                     </goals>
                     <configuration>
                         <tasks>
                            **注意此处配置**
                            **如果模块为以及模块无需配置${parent.artifactId} 最终目录为${target.dir}/${name}**
                            **此处配置不正确会导致jar包移动路径错误，shell脚本无法找到jar包地址**
                            **配置的路径不能重复，每个jar包在独立的文件夹下，多个jar包在一个文件夹下虽能够运行，但gc日志会**
                            **输出到同一个文件内，日志也会混乱**
                             <mkdir dir="${target.dir}/${parent.artifactId}/${name}"/>
                             <copy todir="${target.dir}/${parent.artifactId}/${name}" overwrite="true" >
                                 <fileset dir="${project.build.directory}" erroronmissingdir="false">
                                     <include name="*.jar"/>
                                 </fileset>
                             </copy>
                         </tasks>
                     </configuration>
                 </execution>
             </executions>
         </plugin>
     ```
+ #### 编译说明
  + 编译命令： `mvn clean package -DskipTests=true -DskipDocker -pl <dir> -am `
  + 编译命令需在项目根pom目录下执行，错误的路径maven不能识别构建上下文。    
### 七、服务部署   
+ ####概述
  服务正式环境使用docker容器部署，部署通过build文件夹下的运行脚本进行部署，通过ssh远程部署集群服务。
+ ####运行环境
  + jdk-8u221      配置环境变量
  + docker-ce 18+  配置开机启动
  + Linux内核 4.4   具体查看uname -a
  + 系统设置本地时区，多个服务器间的时间要保持同步
  + firewalld配置开放指定端口
+ ####编译环境
  + maven 配置系统环境变量，maven setting配置公司镜像服务器
  + git   配置免密登陆，通过编辑.git/config 配置文件增加远程地址账号密码
  + 编译服务器配置远程服务器免密SSH连接
+ ####docker运行
  + #####前置条件:
    + 全部运行服务器创建clear用户，用户uid,gid均为1000（或所有服务器clear用户ID保持一致即可）
    + clear用户加入到docker用户组,运行实例也必须使用clear用户。
    + 删除docker0网卡，避免java服务中获取实体机IP混乱，删除docker0网卡后，docker容器运行网络将不能使用bridge模式， \
      只能使用host模式，由于全部运行实例均使用host模式，暂时以此种方式解决。
    + docker配置本地镜像库,编辑/etc/docker/daemon.json
         ```
         {
            "insecure-registries": ["10.150.18.14:6000" ],
            "dns":["114.114.114.114"]
         }
         ```
      docker更改设置需要重启docker服务。命令参考 `systemctl daemon-reload && systemctl restart docker`
      如果不使用远程镜像库,可以选择不配置,所有服务器都在本地制作基础镜像；
    + 创建服务jar包目录，默认设置为/home/clear/project,jar包按照模块分目录存储，文件夹必须为clear用户创建。
  + #####运行相关:
    + 实例运行命令: 全部服务运行命令在/build/run文件夹下run.sh中找到，或者从start.sh中拼接；
        ```
        docker run -d --net=host --name=admin --restart=always --memory=2G --memory-swap=3G --init \
        --volume /home/clear/project/center/admin:/home/clear/program/  alpine-jdk:latest \
        java -jar -Xms2g -Xmx2g -Xmn1024m -XX:SurvivorRatio=3 -XX:+UseParallelGC -XX:ParallelGCThreads=10 -XX:MetaspaceSize=128M \
        -Xloggc:'/home/clear/program/gc.log' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCCause -Duser.timezone=GMT+8 \
        /home/clear/program/datacenter-admin.jar --spring.profiles.active=test
        命令解释：
        -d 守护进程运行
        --net=host 网络使用host模式，使用宿主机网卡，注意由于实例共享宿主机网卡，端口号不能冲突
        --name=admin 容器名称为admin
        --restart=always 设置容器自动重启，注意实例运行健康状态防止循环重启。
        --memory=2G 设置容器可使用最大内存
        --memory-swap=3G 设置容器内存包含交换区大小，此值一定要大于memory
        --init 使用tini启动，防止PID1僵尸进程，防止jvm使用pid1
        --volume 挂载容器运行外部目录，将/home/clear/project/center/admin 挂载到容器中的/home/clear/program/目录
        alpine-jdk:latest 运行的docker镜像
        其余部分为jvm参数，以及环境变量，不再解释
        ```
    + 实例运行常用命令：
    
      命令 | 解释
      ---|---
      docker ps | 查看全部运行中实例
      docker ps -a | 查看全部实例
      docker images | 查看全部镜像
      docker stats | 查看运行状态
      docker stop {容器名称}  | 停止
      docker restart {容器名称} | 重启
      docker start {容器名称} | 启动
      docker rm {容器名称} | 删除
      docker inspect {容器名称} | 查看信息
      docker logs -f --tail=200 {容器名称} | 查看日志
      docker exec -it {容器名称} /bin/bash  |  进入容器
  + ####docker镜像说明
    + **docker镜像构建** 
      + 构建基础镜像的dockerfile全部在build/docker 文件夹下，dockerfile内有注释说明构建步骤\
      + 主要镜像有三个，alpine-glibc、alpine-jdk、baseimage
      + alpine-glibc镜像配置了jre1.8 glibc包以及bash 设置了时区
      + alpine-jdk镜像与alpine-glibc类似，配置了jdk1.8 glibc包以及bash 设置了时区,两个alpine镜像都根据alpine构建\
        最小安装，没有包含字体以及其他服务。
      + baseimage基于alpine-jdk镜像构建，在基础上设置了运行目录，运行用户（clear） 一般服务都使用baseimage映射外部目录运行容器实例。\
      + 全部镜像均推送到了远程docker镜像仓库，如果服务器要使用远程镜像，必须在docker中配置私有仓库镜像地址，配置方式在以上文档中有说明\
    + **构建镜像特殊说明**
      + 根据文档前面说明，所有服务器均删除了docker0网卡，docker在缺少docker0状态下无法构建新的镜像，此时有两种解决办法：\
        在一台docker服务器中创建docker0网卡，在完成构建镜像后删除即可\
        或者在额外一台docker服务器中构建镜像，将镜像推送到远程镜像仓库，供其他服务器使用。\
        目前的基础镜像能够满足绝大多数java程序使用，如果需要特殊的镜像可以从阿里，网易或者dockerhub上寻找合适的镜像\
    + **服务完整镜像**
      + 完整镜像构建需要在POM中配置**docker-maven-plugin**插件。插件的具体配置参考目前配置，当前插件配置适用于全部项目模块\
      + 单独模块完整镜像构建dockerfile在模块目录的docker文件夹下\
        完整dockerfile示例：
          ``` 
          # FROM基础镜像，此处根据场景选择baseimage或者其他基础镜像
          FROM 10.150.18.14:6000/alpine-glibc:latest
          WORKDIR /home/clear/program
          COPY *.jar /home/clear/program/app.jar
          ENV APP app.jar
          RUN set -eux; \
              addgroup --gid 1000 -S clear; \
              adduser clear -S -u 1000 -G clear -h /home/clear/ -s /bin/bash -D;
          RUN chmod +x /home/clear/program -R; \
              chgrp clear /home/clear -R; \
              chown clear /home/clear -R;
          USER clear
          CMD java -jar /home/clear/program/$APP
        ```
        dockerfile CMD中的命令可在docker run 创建容器时覆盖，自定义传入环境变量
        
### 八、ELK
+ ####elasticsearch
  + es集群：
    + 集群概况：集群部署在173、174、175服务器，服务器安装JDK配置系统环境变量，安装nfs、rpcbind服务，并配置开机启动。
    + 集群配置：173、174、175三台服务器均创建了elastic用户运行es服务器，es服务在elastic用户目录下。\
      三台服务器每个运行一个ES进程实例，每个实例均为数据节点,173与175服务器为候选master节点。当前服务运行正常状态下master节点为173服务器 \
    + 集群运行：`/home/elastic/elasticsearch-6.8.1/bin` 内为运行脚本 \
              `/home/elastic/elasticsearch-6.8.1/config` 内为配置文件，elasticsearch.yml为ES配置文件，jvm.options为运行时JVM配置\
              elasticsearch.yml配置了集群名称，节点名称，host，端口等信息。jvm.options配置了最大运行内存12G;
              **以上配置文件在服务器中可以看到，并有注释，或者查看文档**
    +            
    + 
  + es索引：
    + 默认设置：默认创建索引副本数为1，分片数为5。索引如果不想用默认设置，可在索引settings中配置自定义值，参考方式。
        ```
          "settings" : {
            "index" : {
              "number_of_shards" : "1",
              "number_of_replicas" : "1"
            }
          }
      ```
      **详细配置可查看官方文档**
    + 特殊索引设置：在ES集群中zipkin*,logstash*配置了索引模板,通过以下方式查看具体配置\
      `GET _template/logstash:setting_default` \
      `GET _template/zipkin:setting_default` \
      在zipkin logstash创建索引时会应用匹配到的索引模板。日志类索引几乎没有UPDATE操作且数量巨大，每个索引一个分片以减少lucene文件句柄，降低操作系统压力；
    + 索引维护
      + 历史数据索引可以定期进行forcemerge 减少分片segment数量，增加查询性能与查询时的内存消耗。\
        `POST /${index_name}/_forcemerge?max_num_segments=1` 设置segement合并数为1\
      + 常用API
          ```
          #查看每个节点的force_merge线程数
          GET _cat/thread_pool/force_merge?v
          #查看当前哪些索引正在被force_merge
          GET /_cat/indices/?s=segmentsCount:desc&v&h=index,segmentsCount,segmentsMemory,memoryTotal,mergesCurrent,mergesCurrentDocs,storeSize,p,r
          #查看force_merge任务详情
          GET _tasks?detailed=true&actions=*forcemerge
        ```
      + 索引red情况处理：\
        重启ES节点造成索引red情况，只需等待ES重建索引即可。
        如果长时间未能恢复，`GET _cluster/allocation/explain?pretty`查看分配失败原因；
        一般失败原因多为出现UNASSIGNED主分片shards ，通过`GET _cluster/health?pretty&level=shards`查找未分配shards\
        使用`POST _cluster/reroute` API修复 （此API谨慎使用，具体使用可查看官方文档） \
        其他情况可根据不同的失败原因查找对应的解决方案。\
        如果不能恢复，可暂时关闭问题索引，恢复集群正常运行。
  + es备份：
    + es集群备份地址为/mnt/elastic/backup目录，此路径通过NFS服务挂载到174，175服务器上。备份路径在
    + 集群备份方式：
      ````
      # 创建集群备份快照仓库，目前已创建完成
      PUT _snapshot/backup
      {
        "type": "fs",
        "settings": {
          "location": "/mnt/elastic/backup",
          "compress": true
        }
      }
      # 查看全部集群备份快照
      GET  _snapshot/backup/_all
      # 创建集群快照,backup
      PUT _snapshot/backup_name/snapshot_name?wait_for_completion=true
      #在backup仓库创建snapshot20200611快照
      PUT _snapshot/backup/snapshot20200611?wait_for_completion=true
      # 恢复快照示例
      PUT _snapshot/backup/snapshot20200611/_restore
  + es运维：
    + 在es集群运行正常（green 或者 yellow），可以通过kibana界面查看运行状态，监控索引状态。常规的集群索引操作都可以在kibana内完成
    + ES集群处于red状态时，kibana一般不能启动，此时需要通过REST接口操作集群。常用的接口：
      ````
      # 查看集群状态
      GET _cluster/health
      # 查看全部索引状态
      GET _cluster/health?pretty&level=indices
      GET _cat/indices?v
      # 查看全部索引分片状态
      GET _cluster/health?pretty&level=shards
    + jvm相关，设置最大内存12G，此值在16G内存服务器上运行不建议增大，理论上ES内存占用实体机一半内存。如果节点压力较大，可以考虑新增节点。
    + 新增节点，可以考虑只增加数据节点，避免新节点成为master节点造成集群的索引重新分配。
+ ####logstash
  + 目前logstash部署在16服务器上，在/home/clear/logstash 目录内，相关的配置文件都可以在config目录下找到
  + logback，日志通过logback发送到logstash服务器，使用http请求方式日志。
    ```
     # 具体配置方式 参考logback-logstash.xml 文件
     <appender name="stash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
    
    ```
  + logstash-sample.conf 配置了日志接收处理，以及存入elasticsearch配置。  
  + 运行注意事项：\
    保证elastic运行状态正常。保证网络端口开放。\
    在ES状态不正常时注意logstash运行情况，有可能在日志大量请求时，服务内存溢出，CPU占用异常。
    
+ ####kibana
  + 部署运行
  
### 九、RabbitMQ
  + 部署:
  + 运行:        


